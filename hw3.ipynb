{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 - Stock Movement Prediction\n",
    "\n",
    "作業檔案：\n",
    "- hw3.ipynb: 作業＆報告\n",
    "- hw3-fb-data-ipynb: 其它資料(facebook)測試 (資料來源：https://www.sharecast.com/index/Nasdaq_100/prices/download)\n",
    "- hw3-nasdaq-data-ipynb: 其它資料(nasdaq100)測試 (資料來源：https://www.sharecast.com/equity/Facebook_Inc/share-prices/download)\n",
    "\n",
    "資料：\n",
    "- train.csv: sp100 訓練資料(2009-2017)\n",
    "- test.csv: sp100 測試資料(2018)\n",
    "- fb-train.csv: facebook inc 訓練資料(2009-2017)\n",
    "- fb-test.csv: facebook inc 測試資料(2018)\n",
    "- nasdaq-train.csv: nasdaq100 訓練資料(2009-2017)\n",
    "- nasdaq-test.csv: nasdaq100 測試資料(2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2264, 6)\n",
      "          Date  Open Price  Close Price  High Price  Low Price      Volume\n",
      "0  02-Jan-2009      902.99       931.80      934.73     899.35  4048270080\n",
      "1  05-Jan-2009      929.17       927.45      936.63     919.53  5413910016\n",
      "2  06-Jan-2009      931.17       934.70      943.85     927.28  5392620032\n",
      "3  07-Jan-2009      927.45       906.65      927.45     902.37  4704940032\n",
      "4  08-Jan-2009      905.73       909.73      910.00     896.81  4991549952\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data_path = './train.csv'\n",
    "test_data_path = './test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2264, 4)\n",
      "   Open Price  Close Price  High Price  Low Price\n",
      "0      902.99       931.80      934.73     899.35\n",
      "1      929.17       927.45      936.63     919.53\n",
      "2      931.17       934.70      943.85     927.28\n",
      "3      927.45       906.65      927.45     902.37\n",
      "4      905.73       909.73      910.00     896.81\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "\n",
    "train_df.drop(columns=['Date', 'Volume'], inplace=True) # , 'Volume', 'High Price', 'Low Price'\n",
    "test_df.drop(columns=['Date', 'Volume'], inplace=True) # , 'Volume', 'High Price', 'Low Price'\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Open Price  Close Price  High Price  Low Price  Tomorrow Movement  \\\n",
      "0      902.99       931.80      934.73     899.35                0.0   \n",
      "1      929.17       927.45      936.63     919.53                1.0   \n",
      "2      931.17       934.70      943.85     927.28                0.0   \n",
      "3      927.45       906.65      927.45     902.37                1.0   \n",
      "4      905.73       909.73      910.00     896.81                0.0   \n",
      "\n",
      "   Tomorrow Open  \n",
      "0         929.17  \n",
      "1         931.17  \n",
      "2         927.45  \n",
      "3         905.73  \n",
      "4         909.91  \n",
      "      Open Price  Close Price  High Price  Low Price  Tomorrow Movement  \\\n",
      "2259     2684.22      2683.34     2685.35    2678.13                0.0   \n",
      "2260     2679.09      2680.50     2682.74    2677.96                1.0   \n",
      "2261     2682.10      2682.62     2685.64    2678.91                1.0   \n",
      "2262     2686.10      2687.54     2687.66    2682.69                0.0   \n",
      "2263     2689.15      2673.61     2692.12    2673.61                NaN   \n",
      "\n",
      "      Tomorrow Open  \n",
      "2259        2679.09  \n",
      "2260        2682.10  \n",
      "2261        2686.10  \n",
      "2262        2689.15  \n",
      "2263            NaN  \n"
     ]
    }
   ],
   "source": [
    "# Add the column `Tomorrow Movement` by comparing the `Close Price` with previous days as the training target\n",
    "# Add the column `Tomorrow Open` by shifting the column `Open Price` as one of the new features\n",
    "\n",
    "train_df['Tomorrow Movement'] = np.where(train_df['Close Price'].diff() >= 0, 1, 0)\n",
    "test_df['Tomorrow Movement'] = np.where(test_df['Close Price'].diff() >= 0, 1, 0)\n",
    "\n",
    "train_df['Tomorrow Movement'] = train_df['Tomorrow Movement'].shift(-1)\n",
    "test_df['Tomorrow Movement'] = test_df['Tomorrow Movement'].shift(-1)\n",
    "\n",
    "train_df['Tomorrow Open'] = train_df['Open Price'].shift(-1)\n",
    "test_df['Tomorrow Open'] = test_df['Open Price'].shift(-1)\n",
    "\n",
    "print(train_df.head())\n",
    "print(train_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2245, 10)\n",
      "    Open Price  Close Price  High Price  Low Price  Tomorrow Movement  \\\n",
      "18      868.89       845.14      868.89     844.15                0.0   \n",
      "19      845.69       825.88      851.66     821.67                0.0   \n",
      "20      823.09       825.44      830.78     812.87                1.0   \n",
      "21      825.69       838.51      842.60     821.98                0.0   \n",
      "22      837.77       832.23      851.85     829.18                1.0   \n",
      "\n",
      "    Tomorrow Open     S_10      Corr  Open-Close  Open-Open  \n",
      "18         845.69  840.028 -0.237592       -5.20      23.16  \n",
      "19         823.09  838.242 -0.264066        0.55     -23.20  \n",
      "20         825.69  835.774 -0.501164       -2.79     -22.60  \n",
      "21         837.77  839.103 -0.121338        0.25       2.60  \n",
      "22         831.75  838.302 -0.187328       -0.74      12.08  \n",
      "(233, 10)\n",
      "    Open Price  Close Price  High Price  Low Price  Tomorrow Movement  \\\n",
      "18     2867.23      2853.53     2870.62    2851.48                0.0   \n",
      "19     2832.74      2822.43     2837.75    2818.27                1.0   \n",
      "20     2832.41      2823.81     2839.26    2813.04                0.0   \n",
      "21     2816.45      2821.98     2835.96    2812.70                0.0   \n",
      "22     2808.92      2762.13     2808.92    2759.97                0.0   \n",
      "\n",
      "    Tomorrow Open      S_10      Corr  Open-Close  Open-Open  \n",
      "18        2832.74  2826.260  0.945994       -5.64      19.75  \n",
      "19        2832.41  2830.861  0.745889      -20.79     -34.49  \n",
      "20        2816.45  2832.986  0.540062        9.98      -0.33  \n",
      "21        2808.92  2835.381  0.185381       -7.36     -15.96  \n",
      "22        2741.06  2830.564 -0.300583      -13.06      -7.53  \n"
     ]
    }
   ],
   "source": [
    "# Add other new features `S_10`, `Corr`, `Open-Close`, `Open-Open` (explanation is described below)\n",
    "\n",
    "train_df['S_10'] = train_df['Close Price'].rolling(window=10).mean()\n",
    "train_df['Corr'] = train_df['Close Price'].rolling(window=10).corr(train_df['S_10'])\n",
    "train_df['Open-Close'] = train_df['Open Price'] - train_df['Close Price'].shift(1)\n",
    "train_df['Open-Open'] = train_df['Open Price'] - train_df['Open Price'].shift(1)\n",
    "train_df = train_df.dropna()\n",
    "new_train_df = train_df.iloc[:,:10]\n",
    "\n",
    "print(new_train_df.shape)\n",
    "print(new_train_df.head())\n",
    "\n",
    "test_df['S_10'] = test_df['Close Price'].rolling(window=10).mean()\n",
    "test_df['Corr'] = test_df['Close Price'].rolling(window=10).corr(test_df['S_10'])\n",
    "test_df['Open-Close'] = test_df['Open Price'] - test_df['Close Price'].shift(1)\n",
    "test_df['Open-Open'] = test_df['Open Price'] - test_df['Open Price'].shift(1)\n",
    "test_df = test_df.dropna()\n",
    "new_test_df = test_df.iloc[:,:10]\n",
    "\n",
    "print(new_test_df.shape)\n",
    "print(new_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2245, 9)\n",
      "    Open Price  Close Price  High Price  Low Price  Tomorrow Open     S_10  \\\n",
      "18      868.89       845.14      868.89     844.15         845.69  840.028   \n",
      "19      845.69       825.88      851.66     821.67         823.09  838.242   \n",
      "20      823.09       825.44      830.78     812.87         825.69  835.774   \n",
      "21      825.69       838.51      842.60     821.98         837.77  839.103   \n",
      "22      837.77       832.23      851.85     829.18         831.75  838.302   \n",
      "\n",
      "        Corr  Open-Close  Open-Open  \n",
      "18 -0.237592       -5.20      23.16  \n",
      "19 -0.264066        0.55     -23.20  \n",
      "20 -0.501164       -2.79     -22.60  \n",
      "21 -0.121338        0.25       2.60  \n",
      "22 -0.187328       -0.74      12.08  \n",
      "(2245,)\n",
      "18    0.0\n",
      "19    0.0\n",
      "20    1.0\n",
      "21    0.0\n",
      "22    1.0\n",
      "Name: Tomorrow Movement, dtype: float64\n",
      "-----\n",
      "(233, 9)\n",
      "    Open Price  Close Price  High Price  Low Price  Tomorrow Open      S_10  \\\n",
      "18     2867.23      2853.53     2870.62    2851.48        2832.74  2826.260   \n",
      "19     2832.74      2822.43     2837.75    2818.27        2832.41  2830.861   \n",
      "20     2832.41      2823.81     2839.26    2813.04        2816.45  2832.986   \n",
      "21     2816.45      2821.98     2835.96    2812.70        2808.92  2835.381   \n",
      "22     2808.92      2762.13     2808.92    2759.97        2741.06  2830.564   \n",
      "\n",
      "        Corr  Open-Close  Open-Open  \n",
      "18  0.945994       -5.64      19.75  \n",
      "19  0.745889      -20.79     -34.49  \n",
      "20  0.540062        9.98      -0.33  \n",
      "21  0.185381       -7.36     -15.96  \n",
      "22 -0.300583      -13.06      -7.53  \n",
      "(233,)\n",
      "18    0.0\n",
      "19    1.0\n",
      "20    0.0\n",
      "21    0.0\n",
      "22    0.0\n",
      "Name: Tomorrow Movement, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Divide x and y data\n",
    "\n",
    "data_train_x = new_train_df.drop(columns=['Tomorrow Movement'])\n",
    "data_train_y = new_train_df['Tomorrow Movement']\n",
    "\n",
    "data_test_x = new_test_df.drop(columns=['Tomorrow Movement'])\n",
    "data_test_y = new_test_df['Tomorrow Movement']\n",
    "\n",
    "print(data_train_x.shape)\n",
    "print(data_train_x.head())\n",
    "print(data_train_y.shape)\n",
    "print(data_train_y.head())\n",
    "print('-----')\n",
    "print(data_test_x.shape)\n",
    "print(data_test_x.head())\n",
    "print(data_test_y.shape)\n",
    "print(data_test_y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:\n",
      "0.6806236080178174\n",
      "\n",
      "testing accuracy:\n",
      "0.6995708154506438\n",
      "\n",
      "testing result prob:\n",
      "[[9.99646050e-01 3.53949537e-04]\n",
      " [1.30871704e-02 9.86912830e-01]\n",
      " [9.55789009e-01 4.42109907e-02]\n",
      " [9.93249012e-01 6.75098796e-03]\n",
      " [9.99653096e-01 3.46903675e-04]\n",
      " [9.99996968e-01 3.03189316e-06]\n",
      " [8.31329046e-01 1.68670954e-01]\n",
      " [7.14412426e-02 9.28558757e-01]\n",
      " [1.27243879e-04 9.99872756e-01]\n",
      " [2.83113759e-03 9.97168862e-01]\n",
      " [9.77539345e-01 2.24606553e-02]\n",
      " [9.86851773e-01 1.31482267e-02]\n",
      " [2.02623266e-03 9.97973767e-01]\n",
      " [8.71146376e-01 1.28853624e-01]\n",
      " [9.68646596e-01 3.13534038e-02]\n",
      " [1.37960699e-01 8.62039301e-01]\n",
      " [2.18477456e-02 9.78152254e-01]\n",
      " [1.08120745e-02 9.89187925e-01]\n",
      " [2.77545360e-02 9.72245464e-01]\n",
      " [4.53745154e-01 5.46254846e-01]\n",
      " [1.57663168e-02 9.84233683e-01]\n",
      " [3.71201144e-01 6.28798856e-01]\n",
      " [9.99420736e-01 5.79263970e-04]\n",
      " [9.82233922e-01 1.77660781e-02]\n",
      " [1.77200870e-02 9.82279913e-01]\n",
      " [9.99279116e-01 7.20884439e-04]\n",
      " [7.41259764e-02 9.25874024e-01]\n",
      " [4.77008226e-03 9.95229918e-01]\n",
      " [2.21670585e-01 7.78329415e-01]\n",
      " [2.08002456e-02 9.79199754e-01]\n",
      " [2.97055964e-02 9.70294404e-01]\n",
      " [1.47937235e-01 8.52062765e-01]\n",
      " [2.31246825e-01 7.68753175e-01]\n",
      " [9.85055258e-01 1.49447415e-02]\n",
      " [2.76522452e-01 7.23477548e-01]\n",
      " [7.23365357e-01 2.76634643e-01]\n",
      " [9.99630318e-01 3.69681561e-04]\n",
      " [1.44064221e-01 8.55935779e-01]\n",
      " [3.12814498e-06 9.99996872e-01]\n",
      " [5.62755893e-02 9.43724411e-01]\n",
      " [5.18543202e-01 4.81456798e-01]\n",
      " [2.06992095e-02 9.79300790e-01]\n",
      " [9.43301104e-01 5.66988960e-02]\n",
      " [1.04636189e-02 9.89536381e-01]\n",
      " [9.99996326e-01 3.67353437e-06]\n",
      " [3.68348041e-03 9.96316520e-01]\n",
      " [9.98752780e-01 1.24722023e-03]\n",
      " [4.19995204e-03 9.95800048e-01]\n",
      " [3.98814532e-05 9.99960119e-01]\n",
      " [9.96283599e-01 3.71640109e-03]\n",
      " [6.49504987e-03 9.93504950e-01]\n",
      " [6.76984573e-03 9.93230154e-01]\n",
      " [5.18417970e-03 9.94815820e-01]\n",
      " [3.50542895e-03 9.96494571e-01]\n",
      " [2.01541955e-01 7.98458045e-01]\n",
      " [9.48038408e-01 5.19615923e-02]\n",
      " [5.36956023e-01 4.63043977e-01]\n",
      " [1.05305508e-01 8.94694492e-01]\n",
      " [1.74123351e-02 9.82587665e-01]\n",
      " [4.77483162e-01 5.22516838e-01]\n",
      " [1.11804285e-02 9.88819571e-01]\n",
      " [3.70170187e-02 9.62982981e-01]\n",
      " [1.38227185e-01 8.61772815e-01]\n",
      " [8.51794285e-01 1.48205715e-01]\n",
      " [6.41246228e-01 3.58753772e-01]\n",
      " [9.43752276e-01 5.62477244e-02]\n",
      " [9.68072905e-01 3.19270954e-02]\n",
      " [7.31907421e-02 9.26809258e-01]\n",
      " [6.85230733e-01 3.14769267e-01]\n",
      " [8.54875070e-02 9.14512493e-01]\n",
      " [6.17275670e-02 9.38272433e-01]\n",
      " [5.52650518e-01 4.47349482e-01]\n",
      " [9.11809628e-02 9.08819037e-01]\n",
      " [9.89868319e-01 1.01316815e-02]\n",
      " [3.60737341e-01 6.39262659e-01]\n",
      " [7.44178926e-01 2.55821074e-01]\n",
      " [7.11267055e-01 2.88732945e-01]\n",
      " [5.25162286e-03 9.94748377e-01]\n",
      " [1.20650503e-01 8.79349497e-01]\n",
      " [9.84025530e-01 1.59744697e-02]\n",
      " [7.18422177e-01 2.81577823e-01]\n",
      " [8.46099194e-01 1.53900806e-01]\n",
      " [9.98358610e-01 1.64139011e-03]\n",
      " [5.10486912e-03 9.94895131e-01]\n",
      " [8.07884987e-01 1.92115013e-01]\n",
      " [3.92013138e-03 9.96079869e-01]\n",
      " [7.47686040e-02 9.25231396e-01]\n",
      " [3.57865505e-01 6.42134495e-01]\n",
      " [1.47929147e-01 8.52070853e-01]\n",
      " [2.92661319e-01 7.07338681e-01]\n",
      " [8.50338172e-01 1.49661828e-01]\n",
      " [3.81203109e-01 6.18796891e-01]\n",
      " [1.68105974e-01 8.31894026e-01]\n",
      " [4.07182895e-01 5.92817105e-01]\n",
      " [4.22126569e-02 9.57787343e-01]\n",
      " [8.80546296e-01 1.19453704e-01]\n",
      " [9.96042814e-01 3.95718575e-03]\n",
      " [9.99791813e-01 2.08187336e-04]\n",
      " [4.27233851e-02 9.57276615e-01]\n",
      " [3.33471264e-01 6.66528736e-01]\n",
      " [1.35742838e-02 9.86425716e-01]\n",
      " [9.93794487e-01 6.20551281e-03]\n",
      " [1.09551452e-01 8.90448548e-01]\n",
      " [1.12650990e-01 8.87349010e-01]\n",
      " [4.85125082e-01 5.14874918e-01]\n",
      " [1.30838392e-02 9.86916161e-01]\n",
      " [9.93649946e-01 6.35005365e-03]\n",
      " [5.87547434e-02 9.41245257e-01]\n",
      " [1.03083982e-02 9.89691602e-01]\n",
      " [4.78119206e-01 5.21880794e-01]\n",
      " [3.15521952e-02 9.68447805e-01]\n",
      " [1.77285131e-01 8.22714869e-01]\n",
      " [9.96851923e-01 3.14807717e-03]\n",
      " [2.20755436e-02 9.77924456e-01]\n",
      " [6.89058056e-01 3.10941944e-01]\n",
      " [4.74295572e-01 5.25704428e-01]\n",
      " [9.73666414e-01 2.63335858e-02]\n",
      " [2.91151360e-01 7.08848640e-01]\n",
      " [9.19377207e-01 8.06227929e-02]\n",
      " [4.54124992e-01 5.45875008e-01]\n",
      " [7.28550486e-01 2.71449514e-01]\n",
      " [4.14808232e-03 9.95851918e-01]\n",
      " [7.62119536e-01 2.37880464e-01]\n",
      " [9.84740742e-01 1.52592577e-02]\n",
      " [8.59652308e-02 9.14034769e-01]\n",
      " [4.87361857e-01 5.12638143e-01]\n",
      " [5.81029449e-02 9.41897055e-01]\n",
      " [1.38024792e-01 8.61975208e-01]\n",
      " [9.94170169e-01 5.82983080e-03]\n",
      " [2.54981110e-01 7.45018890e-01]\n",
      " [4.97999394e-01 5.02000606e-01]\n",
      " [1.00205662e-01 8.99794338e-01]\n",
      " [6.58766649e-01 3.41233351e-01]\n",
      " [5.32016554e-01 4.67983446e-01]\n",
      " [9.95639639e-01 4.36036094e-03]\n",
      " [2.51229274e-01 7.48770726e-01]\n",
      " [7.72728103e-02 9.22727190e-01]\n",
      " [9.92749070e-01 7.25092983e-03]\n",
      " [5.06672941e-03 9.94933271e-01]\n",
      " [7.64194528e-01 2.35805472e-01]\n",
      " [1.84225960e-01 8.15774040e-01]\n",
      " [1.59307348e-01 8.40692652e-01]\n",
      " [6.84984575e-01 3.15015425e-01]\n",
      " [6.39263209e-01 3.60736791e-01]\n",
      " [9.23076220e-02 9.07692378e-01]\n",
      " [1.95573300e-02 9.80442670e-01]\n",
      " [1.41421488e-01 8.58578512e-01]\n",
      " [2.25847136e-01 7.74152864e-01]\n",
      " [8.89932385e-01 1.10067615e-01]\n",
      " [7.13080480e-01 2.86919520e-01]\n",
      " [8.57436273e-01 1.42563727e-01]\n",
      " [8.80485091e-01 1.19514909e-01]\n",
      " [4.86700293e-01 5.13299707e-01]\n",
      " [9.80342924e-01 1.96570761e-02]\n",
      " [1.75988913e-02 9.82401109e-01]\n",
      " [9.15570791e-01 8.44292086e-02]\n",
      " [4.80212244e-01 5.19787756e-01]\n",
      " [4.55920833e-02 9.54407917e-01]\n",
      " [3.52886307e-01 6.47113693e-01]\n",
      " [6.64106611e-01 3.35893389e-01]\n",
      " [3.35959866e-01 6.64040134e-01]\n",
      " [3.28523102e-01 6.71476898e-01]\n",
      " [1.00504114e-02 9.89949589e-01]\n",
      " [9.92542952e-02 9.00745705e-01]\n",
      " [9.57262656e-01 4.27373437e-02]\n",
      " [2.68953204e-01 7.31046796e-01]\n",
      " [3.63342347e-01 6.36657653e-01]\n",
      " [8.13900225e-02 9.18609978e-01]\n",
      " [8.29238609e-01 1.70761391e-01]\n",
      " [6.64359253e-03 9.93356407e-01]\n",
      " [6.08764576e-01 3.91235424e-01]\n",
      " [3.65613713e-02 9.63438629e-01]\n",
      " [9.30381811e-01 6.96181885e-02]\n",
      " [4.30266067e-01 5.69733933e-01]\n",
      " [9.63838003e-01 3.61619969e-02]\n",
      " [6.83006279e-01 3.16993721e-01]\n",
      " [9.08913757e-01 9.10862426e-02]\n",
      " [9.43018809e-01 5.69811911e-02]\n",
      " [4.00064386e-08 9.99999960e-01]\n",
      " [9.03638613e-01 9.63613870e-02]\n",
      " [9.94950552e-04 9.99005049e-01]\n",
      " [3.71483786e-01 6.28516214e-01]\n",
      " [9.42093818e-01 5.79061822e-02]\n",
      " [4.80448557e-02 9.51955144e-01]\n",
      " [7.61274052e-02 9.23872595e-01]\n",
      " [9.99999054e-01 9.45971382e-07]\n",
      " [6.88230944e-01 3.11769056e-01]\n",
      " [2.84077652e-04 9.99715922e-01]\n",
      " [9.99999798e-01 2.02276483e-07]\n",
      " [2.64316081e-05 9.99973568e-01]\n",
      " [5.56704352e-01 4.43295648e-01]\n",
      " [1.16868828e-04 9.99883131e-01]\n",
      " [6.89573830e-02 9.31042617e-01]\n",
      " [1.34201079e-01 8.65798921e-01]\n",
      " [2.07371393e-01 7.92628607e-01]\n",
      " [5.70026978e-01 4.29973022e-01]\n",
      " [6.98774095e-04 9.99301226e-01]\n",
      " [9.68210399e-01 3.17896009e-02]\n",
      " [9.93098325e-01 6.90167468e-03]\n",
      " [9.44805721e-01 5.51942792e-02]\n",
      " [1.46058714e-01 8.53941286e-01]\n",
      " [1.92394524e-03 9.98076055e-01]\n",
      " [9.65771809e-01 3.42281913e-02]\n",
      " [9.92599301e-01 7.40069907e-03]\n",
      " [8.61962486e-01 1.38037514e-01]\n",
      " [9.99999264e-01 7.36241068e-07]\n",
      " [1.10417554e-03 9.98895824e-01]\n",
      " [9.98640410e-01 1.35959032e-03]\n",
      " [5.71967333e-04 9.99428033e-01]\n",
      " [9.82375883e-01 1.76241167e-02]\n",
      " [1.91168893e-02 9.80883111e-01]\n",
      " [9.47750992e-01 5.22490078e-02]\n",
      " [4.10735755e-01 5.89264245e-01]\n",
      " [7.06573765e-06 9.99992934e-01]\n",
      " [9.77093478e-01 2.29065215e-02]\n",
      " [3.99680289e-15 1.00000000e+00]\n",
      " [9.99999873e-01 1.26821422e-07]\n",
      " [8.98857717e-01 1.01142283e-01]\n",
      " [5.48921828e-01 4.51078172e-01]\n",
      " [4.32940228e-05 9.99956706e-01]\n",
      " [2.57222016e-04 9.99742778e-01]\n",
      " [5.06163923e-02 9.49383608e-01]\n",
      " [9.99771374e-01 2.28626388e-04]\n",
      " [9.58250750e-01 4.17492496e-02]\n",
      " [2.58642554e-03 9.97413574e-01]\n",
      " [4.45649086e-01 5.54350914e-01]\n",
      " [9.65728231e-01 3.42717689e-02]\n",
      " [6.27254892e-01 3.72745108e-01]\n",
      " [9.95881846e-01 4.11815379e-03]\n",
      " [4.01736897e-03 9.95982631e-01]\n",
      " [9.99969380e-01 3.06202454e-05]\n",
      " [9.90891729e-03 9.90091083e-01]\n",
      " [3.48640048e-03 9.96513600e-01]]\n",
      "\n",
      "predicted testing labels:\n",
      "[0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=500)\n",
    "#lr_model = SGDClassifier(loss='log',  max_iter=800)\n",
    "lr_model.fit(data_train_x, data_train_y)\n",
    "\n",
    "predict_train_y = lr_model.predict(data_train_x)\n",
    "print('training accuracy:')\n",
    "print(accuracy_score(data_train_y, predict_train_y))\n",
    "\n",
    "lr_predict_test_y = lr_model.predict(data_test_x)\n",
    "print('\\ntesting accuracy:')\n",
    "print(accuracy_score(data_test_y, lr_predict_test_y))\n",
    "\n",
    "print('\\ntesting result prob:')\n",
    "print(lr_model.predict_proba(data_test_x))\n",
    "\n",
    "print('\\npredicted testing labels:')\n",
    "print(lr_predict_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision, recall, fbeta-score:\n",
      "(0.7017212559295234, 0.6995708154506438, 0.6980889477179236, None)\n",
      "\n",
      "confusion matrix(tn, fp, fn, tp):\n",
      "(72, 42, 28, 91)\n"
     ]
    }
   ],
   "source": [
    "# Print precision, recall, fbeta-score and confusion matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('precision, recall, fbeta-score:')\n",
    "print(precision_recall_fscore_support(data_test_y, lr_predict_test_y, average='weighted'))\n",
    "print('\\nconfusion matrix(tn, fp, fn, tp):')\n",
    "tn, fp, fn, tp = confusion_matrix(data_test_y, lr_predict_test_y).ravel()\n",
    "print((tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Close Price      Corr  High Price  Low Price  Open Price  Open-Close  \\\n",
      "0    -0.984993 -1.076844   -0.963346  -0.977122   -0.952458   -2.492823   \n",
      "1    -1.008416 -1.113485   -0.984314  -1.004493   -0.980653    0.258373   \n",
      "2    -1.008951 -1.441628   -1.009723  -1.015207   -1.008118   -1.339713   \n",
      "3    -0.993056 -0.915948   -0.995339  -1.004115   -1.004958    0.114833   \n",
      "4    -1.000693 -1.007278   -0.984083  -0.995349   -0.990278   -0.358852   \n",
      "\n",
      "   Open-Open      S_10  Tomorrow Open  \n",
      "0   1.572977 -0.979571      -0.981240  \n",
      "1  -1.776734 -0.981731      -1.008700  \n",
      "2  -1.733382 -0.984715      -1.005541  \n",
      "3   0.087428 -0.980690      -0.990863  \n",
      "4   0.772399 -0.981658      -0.998177  \n",
      "[-1 -1  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(data_train_x) #scaler.fit(train_df.append(test_df, ignore_index=True))\n",
    "\n",
    "train_normalize = scaler.transform(data_train_x)\n",
    "train_normalize = np.transpose(train_normalize)\n",
    "\n",
    "normalize_train_x = pd.DataFrame({\n",
    "    'Open Price': train_normalize[0],\n",
    "    'Close Price': train_normalize[1],\n",
    "    'High Price': train_normalize[2],\n",
    "    'Low Price': train_normalize[3],\n",
    "    'Tomorrow Open': train_normalize[4],\n",
    "    'S_10': train_normalize[5],\n",
    "    'Corr': train_normalize[6],\n",
    "    'Open-Close': train_normalize[7],\n",
    "    'Open-Open': train_normalize[8],\n",
    "})\n",
    "\n",
    "test_normalize = scaler.transform(data_test_x)\n",
    "test_normalize = np.transpose(test_normalize)\n",
    "normalize_test_x = pd.DataFrame({\n",
    "    'Open Price': test_normalize[0],\n",
    "    'Close Price': test_normalize[1],\n",
    "    'High Price': test_normalize[2],\n",
    "    'Low Price': test_normalize[3],\n",
    "    'Tomorrow Open': test_normalize[4],\n",
    "    'S_10': test_normalize[5],\n",
    "    'Corr': test_normalize[6],\n",
    "    'Open-Close': test_normalize[7],\n",
    "    'Open-Open': test_normalize[8],\n",
    "})\n",
    "\n",
    "data_train_y = np.where(data_train_y == 0, -1, 1)\n",
    "data_test_y = np.where(data_test_y == 0, -1, 1)\n",
    "\n",
    "print(normalize_train_x.head())\n",
    "print(data_train_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:\n",
      "0.6561247216035635\n",
      "\n",
      "testing accuracy:\n",
      "0.703862660944206\n",
      "[-1  1 -1 -1 -1 -1 -1  1  1  1 -1 -1  1 -1 -1  1  1  1  1 -1  1  1 -1 -1\n",
      "  1 -1  1  1  1  1  1  1  1 -1  1 -1 -1  1  1  1 -1  1 -1  1 -1  1 -1  1\n",
      "  1 -1  1  1  1  1  1 -1 -1  1  1 -1  1  1  1 -1 -1 -1 -1  1 -1  1  1 -1\n",
      "  1 -1  1 -1 -1  1  1 -1 -1 -1 -1  1 -1  1  1  1  1  1 -1  1  1  1  1 -1\n",
      " -1 -1  1  1  1 -1  1  1 -1  1 -1  1  1 -1  1  1 -1  1 -1 -1 -1  1 -1  1\n",
      " -1  1 -1 -1  1 -1  1  1 -1  1 -1  1 -1 -1 -1  1  1 -1  1 -1  1  1 -1 -1\n",
      "  1  1  1  1 -1 -1 -1 -1 -1 -1  1 -1 -1  1  1 -1  1  1  1  1 -1  1  1  1\n",
      " -1  1 -1  1 -1  1 -1 -1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1  1 -1  1 -1  1\n",
      "  1  1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1 -1  1 -1  1 -1  1 -1  1  1 -1  1\n",
      " -1 -1 -1  1  1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC(kernel='linear', C=3000, tol=1e-5)\n",
    "svc_model.fit(normalize_train_x, data_train_y)\n",
    "\n",
    "predict_train_y = svc_model.predict(normalize_train_x)\n",
    "print('training accuracy:')\n",
    "print(accuracy_score(data_train_y, predict_train_y))\n",
    "\n",
    "svc_predict_test_y = svc_model.predict(normalize_test_x)\n",
    "print('\\ntesting accuracy:')\n",
    "print(accuracy_score(data_test_y, svc_predict_test_y))\n",
    "print(svc_predict_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision, recall, fbeta-score:\n",
      "(0.7038341373808904, 0.703862660944206, 0.703731590476021, None)\n",
      "\n",
      "confusion matrix(tn, fp, fn, tp):\n",
      "(78, 36, 33, 86)\n"
     ]
    }
   ],
   "source": [
    "# Print precision, recall, fbeta-score and confusion matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('precision, recall, fbeta-score:')\n",
    "print(precision_recall_fscore_support(data_test_y, svc_predict_test_y, average='weighted'))\n",
    "print('\\nconfusion matrix(tn, fp, fn, tp):')\n",
    "tn, fp, fn, tp = confusion_matrix(data_test_y, svc_predict_test_y).ravel()\n",
    "print((tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2245, 9)\n",
      "   Close Price      Corr  High Price  Low Price  Open Price  Open-Close  \\\n",
      "0    -0.984993 -1.076844   -0.963346  -0.977122   -0.952458   -2.492823   \n",
      "1    -1.008416 -1.113485   -0.984314  -1.004493   -0.980653    0.258373   \n",
      "2    -1.008951 -1.441628   -1.009723  -1.015207   -1.008118   -1.339713   \n",
      "3    -0.993056 -0.915948   -0.995339  -1.004115   -1.004958    0.114833   \n",
      "4    -1.000693 -1.007278   -0.984083  -0.995349   -0.990278   -0.358852   \n",
      "\n",
      "   Open-Open      S_10  Tomorrow Open  \n",
      "0   1.572977 -0.979571      -0.981240  \n",
      "1  -1.776734 -0.981731      -1.008700  \n",
      "2  -1.733382 -0.984715      -1.005541  \n",
      "3   0.087428 -0.980690      -0.990863  \n",
      "4   0.772399 -0.981658      -0.998177  \n",
      "(2245, 2)\n",
      "   0  1\n",
      "0  1  0\n",
      "1  1  0\n",
      "2  0  1\n",
      "3  1  0\n",
      "4  0  1\n"
     ]
    }
   ],
   "source": [
    "left_col = pd.DataFrame(data=np.where(data_train_y == -1, 1, 0)[:])\n",
    "data_train_y = pd.DataFrame(data=np.where(data_train_y == -1, 0, 1)[:])\n",
    "data_train_y = pd.concat( [ left_col, data_train_y ], axis=1, ignore_index=True )\n",
    "\n",
    "left_col = pd.DataFrame(data=np.where(data_test_y == -1, 1, 0)[:])\n",
    "data_test_y = pd.DataFrame(data=np.where(data_test_y == -1, 0, 1)[:])\n",
    "data_test_y = pd.concat( [ left_col, data_test_y ], axis=1, ignore_index=True )\n",
    "\n",
    "print(normalize_train_x.shape)\n",
    "print(normalize_train_x.head())\n",
    "\n",
    "print(data_train_y.shape)\n",
    "print(data_train_y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 418.63055419921875\n",
      "100 415.57806396484375\n",
      "200 414.8939514160156\n",
      "300 414.54803466796875\n",
      "400 414.3395690917969\n",
      "500 414.1991882324219\n",
      "600 414.096923828125\n",
      "700 414.0181579589844\n",
      "800 413.9545593261719\n",
      "900 413.9014587402344\n",
      "1000 413.8555908203125\n",
      "1100 413.81439208984375\n",
      "1200 413.7757568359375\n",
      "1300 413.7384338378906\n",
      "1400 413.7011413574219\n",
      "1500 413.66290283203125\n",
      "1600 413.62298583984375\n",
      "1700 413.5809326171875\n",
      "1800 413.5362854003906\n",
      "1900 413.4889831542969\n",
      "2000 413.43878173828125\n",
      "2100 413.38543701171875\n",
      "2200 413.3291015625\n",
      "2300 413.26959228515625\n",
      "2400 413.2067565917969\n",
      "2500 413.1407165527344\n",
      "2600 413.0715637207031\n",
      "2700 412.9991149902344\n",
      "2800 412.92333984375\n",
      "2900 412.8443908691406\n",
      "3000 412.7621154785156\n",
      "3100 412.6766357421875\n",
      "3200 412.5877990722656\n",
      "3300 412.4957275390625\n",
      "3400 412.40045166015625\n",
      "3500 412.3016357421875\n",
      "3600 412.1995849609375\n",
      "3700 412.09423828125\n",
      "3800 411.98553466796875\n",
      "3900 411.8735046386719\n",
      "4000 411.7581481933594\n",
      "4100 411.6396179199219\n",
      "4200 411.51788330078125\n",
      "4300 411.39300537109375\n",
      "4400 411.2650146484375\n",
      "4500 411.1340637207031\n",
      "4600 411.0003662109375\n",
      "4700 410.86383056640625\n",
      "4800 410.724609375\n",
      "4900 410.5827331542969\n",
      "5000 410.4384765625\n",
      "5100 410.2918701171875\n",
      "5200 410.1429443359375\n",
      "5300 409.9920349121094\n",
      "5400 409.8388977050781\n",
      "5500 409.6839294433594\n",
      "5600 409.5270080566406\n",
      "5700 409.3685607910156\n",
      "5800 409.2082824707031\n",
      "5900 409.0465393066406\n",
      "6000 408.8831787109375\n",
      "6100 408.7184753417969\n",
      "6200 408.5525207519531\n",
      "6300 408.3851623535156\n",
      "6400 408.2167053222656\n",
      "6500 408.047119140625\n",
      "6600 407.87646484375\n",
      "6700 407.70477294921875\n",
      "6800 407.5321350097656\n",
      "6900 407.3586730957031\n",
      "7000 407.1843566894531\n",
      "7100 407.00933837890625\n",
      "7200 406.83343505859375\n",
      "7300 406.6568908691406\n",
      "7400 406.4798278808594\n",
      "7500 406.3020935058594\n",
      "7600 406.1239013671875\n",
      "7700 405.9451599121094\n",
      "7800 405.7658386230469\n",
      "7900 405.5862121582031\n",
      "8000 405.40618896484375\n",
      "8100 405.2257995605469\n",
      "8200 405.045166015625\n",
      "8300 404.86419677734375\n",
      "8400 404.6829528808594\n",
      "8500 404.50146484375\n",
      "8600 404.31988525390625\n",
      "8700 404.1380310058594\n",
      "8800 403.9560852050781\n",
      "8900 403.77410888671875\n",
      "9000 403.5921325683594\n",
      "9100 403.40985107421875\n",
      "9200 403.2276306152344\n",
      "9300 403.0454406738281\n",
      "9400 402.8631896972656\n",
      "9500 402.6810302734375\n",
      "9600 402.49884033203125\n",
      "9700 402.3168029785156\n",
      "9800 402.1348571777344\n",
      "9900 401.9529724121094\n",
      "10000 401.771240234375\n",
      "10100 401.589599609375\n",
      "10200 401.40814208984375\n",
      "10300 401.22698974609375\n",
      "10400 401.0457763671875\n",
      "10500 400.8649597167969\n",
      "10600 400.6842346191406\n",
      "10700 400.50384521484375\n",
      "10800 400.32366943359375\n",
      "10900 400.1436767578125\n",
      "11000 399.9640808105469\n",
      "11100 399.78460693359375\n",
      "11200 399.6054992675781\n",
      "11300 399.4266662597656\n",
      "11400 399.24822998046875\n",
      "11500 399.070068359375\n",
      "11600 398.8921813964844\n",
      "11700 398.7147216796875\n",
      "11800 398.5374450683594\n",
      "11900 398.3607482910156\n",
      "12000 398.1841735839844\n",
      "12100 398.00811767578125\n",
      "12200 397.8323669433594\n",
      "12300 397.656982421875\n",
      "12400 397.482177734375\n",
      "12500 397.3076171875\n",
      "12600 397.13348388671875\n",
      "12700 396.9597473144531\n",
      "12800 396.7864074707031\n",
      "12900 396.61346435546875\n",
      "13000 396.441162109375\n",
      "13100 396.26904296875\n",
      "13200 396.0974426269531\n",
      "13300 395.9263000488281\n",
      "13400 395.7555847167969\n",
      "13500 395.5854187011719\n",
      "13600 395.41552734375\n",
      "13700 395.24627685546875\n",
      "13800 395.077392578125\n",
      "13900 394.9089660644531\n",
      "14000 394.7410583496094\n",
      "14100 394.5735778808594\n",
      "14200 394.4065246582031\n",
      "14300 394.24005126953125\n",
      "14400 394.073974609375\n",
      "14500 393.908447265625\n",
      "14600 393.7433776855469\n",
      "14700 393.5788879394531\n",
      "14800 393.41473388671875\n",
      "14900 393.2510681152344\n",
      "15000 393.0880432128906\n",
      "15100 392.92535400390625\n",
      "15200 392.7632751464844\n",
      "15300 392.6016845703125\n",
      "15400 392.4405822753906\n",
      "15500 392.2799072265625\n",
      "15600 392.1197814941406\n",
      "15700 391.960205078125\n",
      "15800 391.8011474609375\n",
      "15900 391.64251708984375\n",
      "16000 391.4844665527344\n",
      "16100 391.3268737792969\n",
      "16200 391.1698913574219\n",
      "16300 391.0133361816406\n",
      "16400 390.85723876953125\n",
      "16500 390.70166015625\n",
      "16600 390.5466613769531\n",
      "16700 390.3921203613281\n",
      "16800 390.2381896972656\n",
      "16900 390.08465576171875\n",
      "17000 389.9317626953125\n",
      "17100 389.779296875\n",
      "17200 389.6272888183594\n",
      "17300 389.4757995605469\n",
      "17400 389.3249816894531\n",
      "17500 389.17449951171875\n",
      "17600 389.02459716796875\n",
      "17700 388.8752136230469\n",
      "17800 388.7262268066406\n",
      "17900 388.57794189453125\n",
      "18000 388.43017578125\n",
      "18100 388.28277587890625\n",
      "18200 388.1357727050781\n",
      "18300 387.98944091796875\n",
      "18400 387.84368896484375\n",
      "18500 387.6982727050781\n",
      "18600 387.55352783203125\n",
      "18700 387.4092102050781\n",
      "18800 387.26544189453125\n",
      "18900 387.1220397949219\n",
      "19000 386.9792175292969\n",
      "19100 386.8369445800781\n",
      "19200 386.69525146484375\n",
      "19300 386.55377197265625\n",
      "19400 386.4129943847656\n",
      "19500 386.27264404296875\n",
      "19600 386.13287353515625\n",
      "19700 385.9935302734375\n",
      "19800 385.8547668457031\n",
      "19900 385.71636962890625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class M_NN(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(M_NN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.linear1(x)\n",
    "        acti_out = F.relu(h)\n",
    "        y_pred = self.linear2(h) #.clamp(0,1)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size\n",
    "N, D_in, H, D_out = 300, 9, 100, 2\n",
    "\n",
    "model = M_NN(D_in, H, D_out)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(20000):\n",
    "    for batch_num in range(N, len(normalize_train_x), N):\n",
    "        \n",
    "        y_pred = model(torch.FloatTensor(normalize_train_x[batch_num-N:batch_num].values.tolist()))\n",
    "\n",
    "        loss = criterion(y_pred, torch.FloatTensor(data_train_y[batch_num-N:batch_num].values.tolist()))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (t%100 == 0):\n",
    "        print(t, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:\n",
      "0.5826280623608018\n",
      "\n",
      "testing accuracy:\n",
      "0.5879828326180258\n",
      "predicted testing prob:\n",
      "tensor([[ 8.6088e-01, -8.6018e-01],\n",
      "        [-1.6110e+00,  1.6121e+00],\n",
      "        [-3.6443e-01,  3.6419e-01],\n",
      "        [ 3.7257e-01, -3.7143e-01],\n",
      "        [ 8.9495e-01, -8.9405e-01],\n",
      "        [ 2.2214e+00, -2.2190e+00],\n",
      "        [-4.3752e-01,  4.3911e-01],\n",
      "        [-1.4441e+00,  1.4464e+00],\n",
      "        [-3.0387e+00,  3.0390e+00],\n",
      "        [-2.8699e+00,  2.8675e+00],\n",
      "        [-3.8798e-01,  3.8795e-01],\n",
      "        [ 3.9464e-03, -3.2797e-03],\n",
      "        [-2.2361e+00,  2.2372e+00],\n",
      "        [-9.6622e-01,  9.6512e-01],\n",
      "        [ 2.1817e-01, -2.1661e-01],\n",
      "        [-1.0329e+00,  1.0339e+00],\n",
      "        [-1.3235e+00,  1.3250e+00],\n",
      "        [-1.6270e+00,  1.6276e+00],\n",
      "        [-1.7154e+00,  1.7152e+00],\n",
      "        [-1.0837e+00,  1.0836e+00],\n",
      "        [-1.7026e+00,  1.7034e+00],\n",
      "        [-8.5092e-01,  8.5128e-01],\n",
      "        [ 8.2784e-01, -8.2737e-01],\n",
      "        [ 1.5480e-01, -1.5382e-01],\n",
      "        [-1.8293e+00,  1.8304e+00],\n",
      "        [ 3.5227e-01, -3.5294e-01],\n",
      "        [-1.3031e+00,  1.3039e+00],\n",
      "        [-2.2266e+00,  2.2261e+00],\n",
      "        [-1.3090e+00,  1.3089e+00],\n",
      "        [-1.7671e+00,  1.7673e+00],\n",
      "        [-1.6206e+00,  1.6208e+00],\n",
      "        [-1.2392e+00,  1.2390e+00],\n",
      "        [-1.0594e+00,  1.0596e+00],\n",
      "        [ 1.5304e-01, -1.5248e-01],\n",
      "        [-1.2152e+00,  1.2151e+00],\n",
      "        [-5.7561e-01,  5.7580e-01],\n",
      "        [ 1.1097e+00, -1.1082e+00],\n",
      "        [-1.0828e+00,  1.0840e+00],\n",
      "        [-3.5598e+00,  3.5603e+00],\n",
      "        [-1.9326e+00,  1.9312e+00],\n",
      "        [-1.2548e+00,  1.2546e+00],\n",
      "        [-1.4921e+00,  1.4929e+00],\n",
      "        [-2.4991e-01,  2.5072e-01],\n",
      "        [-2.1967e+00,  2.1965e+00],\n",
      "        [ 1.7801e+00, -1.7805e+00],\n",
      "        [-2.0280e+00,  2.0298e+00],\n",
      "        [ 3.9325e-01, -3.9324e-01],\n",
      "        [-1.9392e+00,  1.9401e+00],\n",
      "        [-2.6657e+00,  2.6670e+00],\n",
      "        [ 2.7443e-01, -2.7471e-01],\n",
      "        [-1.6444e+00,  1.6458e+00],\n",
      "        [-1.8621e+00,  1.8624e+00],\n",
      "        [-2.1610e+00,  2.1604e+00],\n",
      "        [-2.0606e+00,  2.0604e+00],\n",
      "        [-1.1998e+00,  1.1999e+00],\n",
      "        [-1.8370e-01,  1.8398e-01],\n",
      "        [-7.8538e-01,  7.8552e-01],\n",
      "        [-1.3512e+00,  1.3511e+00],\n",
      "        [-1.7640e+00,  1.7637e+00],\n",
      "        [-1.1317e+00,  1.1312e+00],\n",
      "        [-1.9013e+00,  1.9008e+00],\n",
      "        [-1.6604e+00,  1.6605e+00],\n",
      "        [-1.4474e+00,  1.4469e+00],\n",
      "        [-3.5984e-01,  3.6032e-01],\n",
      "        [-7.7967e-01,  7.7933e-01],\n",
      "        [-2.0783e-01,  2.0830e-01],\n",
      "        [-2.9147e-01,  2.9101e-01],\n",
      "        [-1.3843e+00,  1.3853e+00],\n",
      "        [-7.5430e-01,  7.5466e-01],\n",
      "        [-1.4277e+00,  1.4276e+00],\n",
      "        [-1.4872e+00,  1.4873e+00],\n",
      "        [-8.5000e-01,  8.5021e-01],\n",
      "        [-1.3342e+00,  1.3345e+00],\n",
      "        [ 2.0799e-01, -2.0768e-01],\n",
      "        [-9.1068e-01,  9.1094e-01],\n",
      "        [-5.2895e-01,  5.2941e-01],\n",
      "        [-6.1095e-01,  6.1154e-01],\n",
      "        [-1.9865e+00,  1.9866e+00],\n",
      "        [-1.3301e+00,  1.3301e+00],\n",
      "        [ 1.3297e-02, -1.3135e-02],\n",
      "        [-5.6967e-01,  5.7022e-01],\n",
      "        [-7.4282e-01,  7.4235e-01],\n",
      "        [ 5.8633e-01, -5.8601e-01],\n",
      "        [-1.9628e+00,  1.9633e+00],\n",
      "        [-5.8893e-01,  5.8904e-01],\n",
      "        [-2.1497e+00,  2.1498e+00],\n",
      "        [-1.4720e+00,  1.4718e+00],\n",
      "        [-1.0679e+00,  1.0679e+00],\n",
      "        [-1.3145e+00,  1.3144e+00],\n",
      "        [-1.1557e+00,  1.1557e+00],\n",
      "        [-5.9408e-01,  5.9409e-01],\n",
      "        [-9.1614e-01,  9.1652e-01],\n",
      "        [-1.1480e+00,  1.1485e+00],\n",
      "        [-1.0127e+00,  1.0126e+00],\n",
      "        [-1.5248e+00,  1.5251e+00],\n",
      "        [-4.6375e-01,  4.6369e-01],\n",
      "        [ 2.2826e-01, -2.2831e-01],\n",
      "        [ 1.0102e+00, -1.0096e+00],\n",
      "        [-1.4832e+00,  1.4841e+00],\n",
      "        [-1.1106e+00,  1.1106e+00],\n",
      "        [-1.8867e+00,  1.8866e+00],\n",
      "        [ 2.2698e-01, -2.2716e-01],\n",
      "        [-1.4501e+00,  1.4500e+00],\n",
      "        [-1.3068e+00,  1.3069e+00],\n",
      "        [-8.1600e-01,  8.1700e-01],\n",
      "        [-1.7974e+00,  1.7978e+00],\n",
      "        [ 2.2079e-01, -2.2012e-01],\n",
      "        [-1.4413e+00,  1.4419e+00],\n",
      "        [-2.0040e+00,  2.0041e+00],\n",
      "        [-1.0121e+00,  1.0117e+00],\n",
      "        [-1.6616e+00,  1.6619e+00],\n",
      "        [-1.2867e+00,  1.2866e+00],\n",
      "        [ 3.9706e-01, -3.9694e-01],\n",
      "        [-1.4755e+00,  1.4764e+00],\n",
      "        [-7.4071e-01,  7.4064e-01],\n",
      "        [-9.1900e-01,  9.1924e-01],\n",
      "        [-9.1959e-02,  9.2102e-02],\n",
      "        [-9.4043e-01,  9.4138e-01],\n",
      "        [-4.7033e-01,  4.7035e-01],\n",
      "        [-8.9844e-01,  8.9882e-01],\n",
      "        [-6.2051e-01,  6.2087e-01],\n",
      "        [-2.1105e+00,  2.1107e+00],\n",
      "        [-8.0144e-01,  8.0121e-01],\n",
      "        [ 3.9589e-02, -3.8884e-02],\n",
      "        [-1.3067e+00,  1.3077e+00],\n",
      "        [-1.0744e+00,  1.0741e+00],\n",
      "        [-1.4631e+00,  1.4633e+00],\n",
      "        [-1.2655e+00,  1.2657e+00],\n",
      "        [ 1.2338e-01, -1.2337e-01],\n",
      "        [-1.0580e+00,  1.0588e+00],\n",
      "        [-1.0453e+00,  1.0454e+00],\n",
      "        [-1.4377e+00,  1.4379e+00],\n",
      "        [-7.6839e-01,  7.6864e-01],\n",
      "        [-8.4270e-01,  8.4306e-01],\n",
      "        [ 3.4599e-01, -3.4549e-01],\n",
      "        [-1.0678e+00,  1.0683e+00],\n",
      "        [-1.4114e+00,  1.4118e+00],\n",
      "        [ 1.5166e-01, -1.5148e-01],\n",
      "        [-2.1969e+00,  2.1969e+00],\n",
      "        [-6.4586e-01,  6.4608e-01],\n",
      "        [-1.2409e+00,  1.2413e+00],\n",
      "        [-1.3371e+00,  1.3372e+00],\n",
      "        [-6.5744e-01,  6.5798e-01],\n",
      "        [-7.4898e-01,  7.4934e-01],\n",
      "        [-1.3410e+00,  1.3415e+00],\n",
      "        [-1.8140e+00,  1.8141e+00],\n",
      "        [-1.4336e+00,  1.4336e+00],\n",
      "        [-1.2685e+00,  1.2685e+00],\n",
      "        [-4.7706e-01,  4.7732e-01],\n",
      "        [-7.3645e-01,  7.3686e-01],\n",
      "        [-5.0774e-01,  5.0806e-01],\n",
      "        [-5.6479e-01,  5.6488e-01],\n",
      "        [-1.0175e+00,  1.0176e+00],\n",
      "        [-1.7238e-01,  1.7239e-01],\n",
      "        [-1.6332e+00,  1.6342e+00],\n",
      "        [-4.6393e-01,  4.6396e-01],\n",
      "        [-9.0991e-01,  9.1047e-01],\n",
      "        [-1.6860e+00,  1.6861e+00],\n",
      "        [-1.0963e+00,  1.0963e+00],\n",
      "        [-8.7502e-01,  8.7484e-01],\n",
      "        [-1.0530e+00,  1.0531e+00],\n",
      "        [-9.7776e-01,  9.7829e-01],\n",
      "        [-1.9601e+00,  1.9603e+00],\n",
      "        [-1.4889e+00,  1.4889e+00],\n",
      "        [-3.1000e-01,  3.1015e-01],\n",
      "        [-1.1126e+00,  1.1129e+00],\n",
      "        [-1.0786e+00,  1.0787e+00],\n",
      "        [-1.3633e+00,  1.3641e+00],\n",
      "        [-5.1357e-01,  5.1415e-01],\n",
      "        [-1.9905e+00,  1.9910e+00],\n",
      "        [-9.4301e-01,  9.4302e-01],\n",
      "        [-1.6091e+00,  1.6095e+00],\n",
      "        [-3.9751e-01,  3.9767e-01],\n",
      "        [-1.1394e+00,  1.1392e+00],\n",
      "        [-3.2258e-01,  3.2254e-01],\n",
      "        [-8.3223e-01,  8.3240e-01],\n",
      "        [-4.3208e-01,  4.3276e-01],\n",
      "        [-3.6928e-01,  3.7015e-01],\n",
      "        [-4.5105e+00,  4.5112e+00],\n",
      "        [-1.2379e+00,  1.2351e+00],\n",
      "        [-2.4151e+00,  2.4159e+00],\n",
      "        [-1.2624e+00,  1.2624e+00],\n",
      "        [-7.5765e-01,  7.5701e-01],\n",
      "        [-1.6336e+00,  1.6338e+00],\n",
      "        [-1.2877e+00,  1.2885e+00],\n",
      "        [ 2.0938e+00, -2.0936e+00],\n",
      "        [-5.9886e-01,  6.0009e-01],\n",
      "        [-2.7981e+00,  2.7988e+00],\n",
      "        [ 2.5675e+00, -2.5671e+00],\n",
      "        [-3.1551e+00,  3.1567e+00],\n",
      "        [-1.3488e+00,  1.3480e+00],\n",
      "        [-2.9041e+00,  2.9044e+00],\n",
      "        [-1.5541e+00,  1.5548e+00],\n",
      "        [-1.4383e+00,  1.4382e+00],\n",
      "        [-1.3782e+00,  1.3780e+00],\n",
      "        [-7.2563e-01,  7.2573e-01],\n",
      "        [-2.4183e+00,  2.4185e+00],\n",
      "        [-2.7567e-01,  2.7563e-01],\n",
      "        [ 2.5646e-01, -2.5589e-01],\n",
      "        [-2.3344e-01,  2.3356e-01],\n",
      "        [-1.1580e+00,  1.1586e+00],\n",
      "        [-1.9719e+00,  1.9728e+00],\n",
      "        [-3.8934e-01,  3.8891e-01],\n",
      "        [ 9.0663e-02, -9.0466e-02],\n",
      "        [-4.8066e-01,  4.8177e-01],\n",
      "        [ 2.1342e+00, -2.1337e+00],\n",
      "        [-1.9115e+00,  1.9134e+00],\n",
      "        [ 5.5317e-01, -5.5293e-01],\n",
      "        [-2.2530e+00,  2.2543e+00],\n",
      "        [-1.9819e-01,  1.9791e-01],\n",
      "        [-1.8312e+00,  1.8315e+00],\n",
      "        [-4.9679e-01,  4.9687e-01],\n",
      "        [-9.6128e-01,  9.6208e-01],\n",
      "        [-3.4583e+00,  3.4584e+00],\n",
      "        [-3.5555e-01,  3.5446e-01],\n",
      "        [-7.9957e+00,  7.9963e+00],\n",
      "        [ 2.1784e+00, -2.1809e+00],\n",
      "        [-3.1653e-01,  3.1673e-01],\n",
      "        [-8.6220e-01,  8.6337e-01],\n",
      "        [-3.4079e+00,  3.4066e+00],\n",
      "        [-3.0855e+00,  3.0845e+00],\n",
      "        [-1.3518e+00,  1.3524e+00],\n",
      "        [ 9.0118e-01, -9.0115e-01],\n",
      "        [ 2.1264e-02, -2.0114e-02],\n",
      "        [-2.1236e+00,  2.1241e+00],\n",
      "        [-1.0153e+00,  1.0149e+00],\n",
      "        [ 2.6871e-02, -2.5215e-02],\n",
      "        [-7.6850e-01,  7.6873e-01],\n",
      "        [ 6.8432e-01, -6.8210e-01],\n",
      "        [-1.7628e+00,  1.7641e+00],\n",
      "        [ 1.0252e+00, -1.0251e+00],\n",
      "        [-2.5692e+00,  2.5690e+00],\n",
      "        [-2.1661e+00,  2.1665e+00]], grad_fn=<AddmmBackward>)\n",
      "predicted testing labels:\n",
      "[1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 0 0 1 0 1 0 1 0 0]\n",
      "precision, recall, fbeta-score:\n",
      "(0.6459997799053593, 0.5879828326180258, 0.5348404909652551, None)\n",
      "\n",
      "confusion matrix(tn, fp, fn, tp):\n",
      "(109, 10, 86, 28)\n"
     ]
    }
   ],
   "source": [
    "nn_predict_train_y = model.forward( torch.FloatTensor(normalize_train_x.values.tolist()))\n",
    "result_train = np.where(nn_predict_train_y[:, 0] > nn_predict_train_y[:, 1], 1, 0)\n",
    "print('training accuracy:')\n",
    "print(accuracy_score(data_train_y[0], result_train))\n",
    "\n",
    "nn_predict_y = model.forward( torch.FloatTensor(normalize_test_x.values.tolist()))\n",
    "result = np.where(nn_predict_y[:, 0] > nn_predict_y[:, 1], 1, 0)\n",
    "print('\\ntesting accuracy:')\n",
    "print(accuracy_score(data_test_y[0], result))\n",
    "\n",
    "print('predicted testing prob:')\n",
    "print(nn_predict_y)\n",
    "print('predicted testing labels:')\n",
    "print(result)\n",
    "\n",
    "# Print precision, recall, fbeta-score and confusion matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('precision, recall, fbeta-score:')\n",
    "print(precision_recall_fscore_support(data_test_y[0], result, average='weighted'))\n",
    "print('\\nconfusion matrix(tn, fp, fn, tp):')\n",
    "tn, fp, fn, tp = confusion_matrix(data_test_y[0], result).ravel()\n",
    "print((tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "## How did you preprocess this dataset ?\n",
    "欄位說明：\n",
    "- data x:\n",
    "    - Open Price: 前一天的開盤價\n",
    "    - Close Price: 前一天的收盤價\n",
    "    - High Price: 前一天的最高價\n",
    "    - Low Price: 前一天的最低價\n",
    "    - Tomorrow Open: 當天的開盤價\n",
    "    - S_10: 前十天的平均收盤價\n",
    "    - Corr: 前一天收盤價與前十天平均收盤價的相關度\n",
    "    - Open-Close: 前一天的開盤價與更前一天的收盤價差\n",
    "    - Open-Open: 前一天的開盤價與更前一天的開盤價差\n",
    "- data y:\n",
    "    - Tomorrow Movement: 隔天的收盤價漲跌(1為漲, 0或-1為跌)\n",
    "\n",
    "三種分類器中，只有 SVM 與 NN 使用 normalize 過的 `data x`\n",
    "\n",
    "## Which classifier reaches the highest classification accuracy in this dataset ?\n",
    "SVM 跑測試資料的 accuracy 最高（可以從上面跑的結果看到各個分類器跑測試資料的 precision, recall, fbeta-score 和 confusion matrix），但其實跑出來的結果也跟 logistic regression 的 accuracy 很接近，可能因為這次使用的 SVM kernel 也是 linear 的。\n",
    "\n",
    "nn 跑出來的結果從 confusion matrix 的 true-negative 跟 false-negative 的數量都很多，就可以知道它大部分都預測收盤價下跌，precision 也接近 0.7 只是因為它都預測 negative 所以 fp 也很少。\n",
    "\n",
    "svm 可以表現比較好可能也因為資料的 feature 很少，不到 10 個，svm 就可以比較容易找到 hyperplane。\n",
    "\n",
    "三個同樣的分類器跑其它股價資料的 accuracy 也都跟跑 sp100 資料的 accuracy 差不多（執行的過程可以看另外兩個檔案： `hw3-nasdaq-data.ipynb` 和 `hw3-fb-data.ipynb`）都是 svm 跟 logistic regression 表現差不多，accuracy 都接近 0.7，而 nn 的 accuracy 最低。\n",
    "\n",
    "- 跑fb資料結果（訓練資料1400筆左右）：\n",
    "    - logistic regression\n",
    "        - training accuracy:\n",
    "        0.6437275985663082\n",
    "\n",
    "        - testing accuracy:\n",
    "        0.6824034334763949\n",
    "        \n",
    "        - precision, recall, fbeta-score:\n",
    "        (0.6849727015269627, 0.6824034334763949, 0.6804216988464749, None)\n",
    "\n",
    "        - confusion matrix(tn, fp, fn, tp):\n",
    "        (69, 45, 29, 90)\n",
    "    - svm\n",
    "        - training accuracy:\n",
    "        0.6379928315412187\n",
    "        \n",
    "        - testing accuracy:\n",
    "        0.6824034334763949\n",
    "        \n",
    "        - precision, recall, fbeta-score:\n",
    "        (0.6831712054788209, 0.6824034334763949, 0.6815222651392658, None)\n",
    "\n",
    "        - confusion matrix(tn, fp, fn, tp):\n",
    "        (72, 42, 32, 87)\n",
    "    - nn\n",
    "        - training accuracy:\n",
    "        0.617921146953405\n",
    "\n",
    "        - testing accuracy:\n",
    "        0.6437768240343348\n",
    "\n",
    "        - precision, recall, fbeta-score:\n",
    "        (0.6145076659593196, 0.6094420600858369, 0.6021943961339545, None)\n",
    "\n",
    "        - confusion matrix(tn, fp, fn, tp):\n",
    "        (88, 31, 60, 54)\n",
    "- 跑nasdaq100資料結果（訓練資料2250筆左右）：\n",
    "    - logistic regression\n",
    "        - training accuracy:\n",
    "        0.6898395721925134\n",
    "\n",
    "        - testing accuracy:\n",
    "        0.6952789699570815\n",
    "        \n",
    "        - precision, recall, fbeta-score:\n",
    "        (0.6953338947871519, 0.6952789699570815, 0.6942611409862763, None)\n",
    "\n",
    "        - confusion matrix(tn, fp, fn, tp):\n",
    "        (71, 40, 31, 91)\n",
    "    - svm\n",
    "        - training accuracy:\n",
    "        0.6836007130124777\n",
    "\n",
    "        - testing accuracy:\n",
    "        0.6952789699570815\n",
    "        \n",
    "        - precision, recall, fbeta-score:\n",
    "        (0.695036719122556, 0.6952789699570815, 0.6950423721854327, None)\n",
    "\n",
    "        - confusion matrix(tn, fp, fn, tp):\n",
    "        (74, 37, 34, 88)\n",
    "    - nn\n",
    "        - training accuracy:\n",
    "        0.625222816399287\n",
    "\n",
    "        - testing accuracy:\n",
    "        0.6137339055793991\n",
    "\n",
    "        - precision, recall, fbeta-score:\n",
    "        (0.6714440817799076, 0.6137339055793991, 0.5650350410170044, None)\n",
    "\n",
    "        - confusion matrix(tn, fp, fn, tp):\n",
    "        (113, 9, 81, 30)\n",
    "        \n",
    "## How did you improve your classifiers ?\n",
    "- logistic regression：\n",
    "    把參數`max_iter`調大，讓它把訓練資料多訓練幾次，accuracy 會有比較明顯的增加。\n",
    "\n",
    "- svm：\n",
    "    試過各個 kernel function，發現 `linear` kernel 表現較好，將`C`值調大，讓它分錯資料時會有較大的 error，可以找到較適合區分資料的 hyperplane。\n",
    "\n",
    "- nn：\n",
    "    一開始會一直都預測 0，試過一些 activation layer，最後加了一層 relu，再調整 batch_size, hidden_size 跟調大 iteration 次數，它預測1的次數才有多一點點但是 accuracy 還是跟 svm 或 logistic regression 差很多。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
